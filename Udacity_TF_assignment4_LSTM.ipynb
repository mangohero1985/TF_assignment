{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat a small validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "['ate social ', 'ments faile', 'al park pho', 'ies index s', 'ess of cast', ' h provided', 'guage among', 'gers in dec', 'al media an', ' during the', 'known manuf', 'seven a wid', 's covering ', 'en one of t', 'ze single a', ' first card', ' in jersey ', 'he poverty ', 'gns of huma', ' cause so a', 'n denatural', 'ce formatio', 'the input u', 'ck to pull ', 'usion inabi', 'omplete an ', 't of the mi', ' it fort de', 'ttempts by ', 'ormats for ', 'soteric chr', 'growing pop', 'riginal doc', 'e nine eigh', 'rch eight l', 'haracter li', 'al mechanic', ' gm compari', 's fundament', 'lieve the c', 'ast not par', ' upon by hi', ' example rl', 'ed on the w', 'he official', 'on at this ', 'ne three tw', 'inux enterp', ' daily coll', 'ration camp', 'ehru wished', 'stiff from ', 'arman s syd', 'o to begin ', 'itiatives t', 'these autho', 'icky ricard', 'w of mathem', 'ent of arm ', 'credited pr', 'e external ', ' other stat', ' buddhism e', 'vices possi']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295860 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.00\n",
      "================================================================================\n",
      "yf eteqaguk x bpujlv im ys x  le  njch smjhvkplecltevagnqdvfejxber nu klae msgtx\n",
      "zknhtnccppfs oitb zon pfjehfgjd mlplgtex lo  mr tevtq  iijhnhx  qvhiwlvaje b s i\n",
      "idioibf   i nqt szchba bgh  s yfiocgoo acfoizesi lalasss enilflhr  q ucs fszetqh\n",
      "jbeomm nhlgntm axxbajymlsrmsqdz njvmkgj oa zhfkclmqtxlie cgddsiie nc brofzkrms h\n",
      "gu  sigpesou giroiikutn hociois trqhambojoqkejvsgcxorcrg c ciol k sjmqn dbsvftsc\n",
      "================================================================================\n",
      "Validation set perplexity: 19.86\n",
      "Average loss at step 100: 2.591091 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.54\n",
      "Validation set perplexity: 10.47\n",
      "Average loss at step 200: 2.236416 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.77\n",
      "Validation set perplexity: 8.70\n",
      "Average loss at step 300: 2.072804 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.07\n",
      "Validation set perplexity: 8.03\n",
      "Average loss at step 400: 1.992476 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.44\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 500: 1.993129 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.93\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 600: 1.921292 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.66\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 700: 1.893875 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 800: 1.871883 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 900: 1.852382 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 1000: 1.789329 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "d bu if the inlew to pener to and wall origal wartheor stain the boow mostcliol \n",
      "orting of y ware whith reportitions enter martral andyusen prefame of blainative\n",
      "phe provies the laighegas as bearke fine nine five nine one nine five zero zero \n",
      "gred the crist inderies hett beex was jable in bridies present six enternine sta\n",
      "kor or horovel it previch suberagas one true sear one six eight ailigre careactd\n",
      "================================================================================\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1100: 1.766678 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 1200: 1.789399 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1300: 1.768427 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1400: 1.737974 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 1500: 1.732705 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1600: 1.718053 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 1700: 1.737445 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1800: 1.699714 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1900: 1.706550 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2000: 1.711778 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "================================================================================\n",
      "ch sovile glow and u exyally baysledsen of afmiclian the sibjer in the cansspan \n",
      "dry wenerau wings almbbotaise company givern this mychandeinon mirche and lave i\n",
      "tom degased tice whotine from or tedterope at the first enget with assemestancci\n",
      "n genevence and for the eailitic stichlicines bning ard to distril aticles proje\n",
      "encen internate acticinination eed massorined in the lows it is hisd dividected \n",
      "================================================================================\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2100: 1.699721 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 2200: 1.666758 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2300: 1.678805 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2400: 1.678828 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2500: 1.697480 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2600: 1.672060 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2700: 1.687830 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2800: 1.647374 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2900: 1.653524 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3000: 1.657355 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "================================================================================\n",
      "is huntopt to enoim the two seven of on sas decearben discorg a madely these he \n",
      "x fere heaks which used three prosound a mation zero zero k with the orgical tra\n",
      "ize denex ets zero eighted five euched of gene is used shaplate of powing the ne\n",
      "ques under entire for chidami lizes ton for was in beed in the shmether pertay s\n",
      "ther lomant it of the polime necoending as thors throughviy entirity two mecrimg\n",
      "================================================================================\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3100: 1.651086 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3200: 1.648070 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3300: 1.630828 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3400: 1.630095 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3500: 1.626753 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3600: 1.628046 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3700: 1.628507 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3800: 1.619820 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3900: 1.615767 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 4000: 1.618116 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "================================================================================\n",
      "cipents imalion one three seven zero jure won the sempterm symbolle cup probuban\n",
      "ks sevenshalds one cultronch alsoonley or tese insiging gooda the medilit intenc\n",
      "t byads one nine one one zero was which and baindok say prefice or in the fame o\n",
      "nen thiphan on ghouchogic whan have reform vi thausyforting wive language that h\n",
      "ada airised in did led when and junk durine others in the genemences of the mays\n",
      "================================================================================\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4100: 1.623328 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4200: 1.602988 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4300: 1.591077 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 4400: 1.620897 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4500: 1.623970 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4600: 1.625558 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 4700: 1.593088 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4800: 1.576122 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4900: 1.593785 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 5000: 1.620223 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "================================================================================\n",
      "ifer on hreet is a good and mblazoz wassed to culates like of a clanies was of e\n",
      "ne he consider the isly fion heads other stude which resurtia de eny yeary racip\n",
      "was opponial the handbuls dana be the must science was classegraw actral was cyn\n",
      " pholit bi r it abewropine relahe for former c ids subrecession coppitante hal w\n",
      "kh of symit be the tmor used when be pees are bilk for the eastray sugrests prev\n",
      "================================================================================\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 5100: 1.629224 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 5200: 1.623077 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5300: 1.586405 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5400: 1.585275 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5500: 1.577790 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5600: 1.603005 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5700: 1.559315 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5800: 1.569152 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5900: 1.583368 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6000: 1.554531 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      " nivern they has and has just in iptrace americ histolio from lingas their two f\n",
      "vision waveling inkenday at to be the ve or per collect for a defentify gales ca\n",
      "diques consputerctures operation devilupate yackar in the energy culture rainey \n",
      "progragan party r his form waten in indistauce and packared tuans off contents b\n",
      "was if restansion heling meanus the groved vieworion and kine of two the in two \n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6100: 1.575170 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6200: 1.592007 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6300: 1.604172 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6400: 1.636118 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6500: 1.630188 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6600: 1.601352 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6700: 1.587753 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6800: 1.572604 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6900: 1.565774 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 7000: 1.573087 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.92\n",
      "================================================================================\n",
      "was seiter trow qurar ire from by claims any scients date the baint of the beach\n",
      "ard the istorned in the same of four has and uso of the norwaked on excetdant in\n",
      "queisk between nelication the nover between feorimer large francon the kost this\n",
      "qut in progons above asgeting throwe three lacule milethens the consciale frc ch\n",
      "gue changer music in secprolics named in carching in orchene refersic huts oftor\n",
      "================================================================================\n",
      "Validation set perplexity: 4.47\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "with tf.Session(graph=graph,config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True,gpu_options=gpu_options)) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem 1\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # contigent matrix: input, previous output, and bias.\n",
    "  x = tf.Variable(tf.truncated_normal([vocabulary_size*2, num_nodes*2], -0.1, 0.1))\n",
    "  m = tf.Variable(tf.truncated_normal([num_nodes*2, num_nodes*2], -0.1, 0.1))\n",
    "  b_sub = tf.Variable(tf.zeros([1*2, num_nodes*2]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, x[:vocabulary_size,:num_nodes]) + tf.matmul(o, m[:num_nodes,:num_nodes]) + b_sub[0,:num_nodes])\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, x[:vocabulary_size,num_nodes:]) + tf.matmul(o, m[:num_nodes,num_nodes:]) + b_sub[0,num_nodes:])\n",
    "    update = tf.matmul(i, x[vocabulary_size:,:num_nodes]) + tf.matmul(o, m[num_nodes:,:num_nodes]) + b_sub[1,:num_nodes]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, x[vocabulary_size:,num_nodes:]) + tf.matmul(o, m[num_nodes:,num_nodes:]) + b_sub[1,num_nodes:])\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294099 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.95\n",
      "================================================================================\n",
      " s neirtmepltobupueplnjafqntvvcbeonvptamlcqvsbvnm volzhm teorp c yexenfcng l ttr\n",
      "pyh  d   oo wvrtnc hynoefndwma sfes  fe supgtp hnxbpdnjme  tiekej dbw lizxnm zby\n",
      "vqbabuxg zbffoy e oinaij  ibkksnoztj xoqc syll  iiq  ouhwqbow emlz iv k cbjzeaps\n",
      "ly ytmqhkeewtbi   zkqtnfqwokvob kntxusguenssxpxeae iodj nbpny gqj bpoimpeaafxet \n",
      "pcitodoes rkypgehm up pbe zlwchwkmllubgotnhwsrib njmywtbsdwlep qzai cetw wntd c \n",
      "================================================================================\n",
      "Validation set perplexity: 20.09\n",
      "Average loss at step 100: 2.607439 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.07\n",
      "Validation set perplexity: 10.55\n",
      "Average loss at step 200: 2.260957 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.15\n",
      "Validation set perplexity: 8.75\n",
      "Average loss at step 300: 2.129093 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.98\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 400: 2.037377 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 7.82\n",
      "Average loss at step 500: 1.966734 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.54\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 600: 1.909577 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 700: 1.888742 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 800: 1.882943 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.43\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 900: 1.852826 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 1000: 1.824769 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "================================================================================\n",
      "ly folles comp halls and is be auction with the zexber bidinos the frobes as fas\n",
      "mad which statly stfori ner of govally or procuced inopermaniak the sangoust and\n",
      "d emflene mallies bulkhed hath stared no seven scrotem ap racks five maary tomal\n",
      "ninurs lugh works it produces one nine eight nuber three of oped agroligirs lad \n",
      "ed communor of babats rode commanal it and boowick dom to digitar are of plisiti\n",
      "================================================================================\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 1100: 1.783277 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 1200: 1.767908 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1300: 1.778813 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1400: 1.761712 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1500: 1.752256 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1600: 1.749658 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1700: 1.729615 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1800: 1.728421 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1900: 1.703950 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 2000: 1.704391 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "================================================================================\n",
      "quarst coll to grajum inso count vically well the freekwong as soiglis incomput \n",
      "hame theoey he the ground which thes the asteretter sting by foun b one zero ris\n",
      "u scemen this includially ling two jansuscouch juclar defention thafphics to hav\n",
      "ingla miniz low cokmun assoman of the pobel and roding the to skarfive qulife pa\n",
      "or destino be veting in his ke the plosubst or zero reberitious morro to workre \n",
      "================================================================================\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2100: 1.713459 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2200: 1.723540 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2300: 1.695284 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2400: 1.684189 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2500: 1.685702 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2600: 1.683381 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2700: 1.685162 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2800: 1.677354 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2900: 1.644060 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3000: 1.662884 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "================================================================================\n",
      "bod s depare s has inalel and by pychrist peindard sectad by the callical audeq \n",
      "pfoneted include sontwe othing enimies deadered quter sofing betweeng weye its a\n",
      "ed askanging these dire ireads many prodby have be west to measing biddd meditin\n",
      "gy that in in will achound with sude groups camusidal which ikd most ortinome na\n",
      "wale sucrate of hend to and a positients stratio m importing religionsly and con\n",
      "================================================================================\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3100: 1.674953 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3200: 1.652531 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3300: 1.649143 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3400: 1.658375 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3500: 1.662073 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3600: 1.614982 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3700: 1.617339 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3800: 1.631970 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3900: 1.642496 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 4000: 1.609645 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.11\n",
      "================================================================================\n",
      "zer as a such e spobing or parting beenwed fima such be five instences there col\n",
      "ernication exca ner doen its ony texp griternave change he forfer tiched hand pr\n",
      "haning uses the metic followity foet tom changer where invertance is is scho of \n",
      "d as exphates ister wulder rom in this is is which coodentials isbn the nez rego\n",
      "my arran and enens the relive later her jamabure rebate would cimoa were zero by\n",
      "================================================================================\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4100: 1.595540 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4200: 1.592195 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4300: 1.591346 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4400: 1.594410 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4500: 1.596942 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4600: 1.567382 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4700: 1.561748 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4800: 1.584917 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4900: 1.574806 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5000: 1.559076 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "ustly the bring an objod a toots and ruritanian intiesed to the many an erric tu\n",
      "fernt will piete and insuct the computporth in buily with cammle and the deet be\n",
      "x five four four moviamented by crainmany to pay souther kife and wost are von o\n",
      "quents and in the prhsoved in ordling chaiges on incluside one nine five two two\n",
      "ly it recould kgom the president geniet diane five two zero zero kn commind the \n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5100: 1.538484 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5200: 1.562687 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5300: 1.546003 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5400: 1.591894 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5500: 1.557478 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5600: 1.550598 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5700: 1.555351 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5800: 1.569176 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5900: 1.588456 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6000: 1.554718 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "ed builfrage main to who and and to yestor and colecsion costing both will of ad\n",
      "crise to player for south flock this with there the in has the viro to highanian\n",
      "nes of was ipel tree co butier bit creasing the cothmain and seems was of the th\n",
      "urles belf pocktool and and applased by is are appears their in cappatic progrol\n",
      "or shoptible patoration acticle for herestabled fiction later types seen zero ze\n",
      "================================================================================\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6100: 1.557399 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6200: 1.601896 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6300: 1.575387 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6400: 1.581825 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6500: 1.594266 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6600: 1.573108 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6700: 1.542754 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6800: 1.529674 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6900: 1.553691 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 7000: 1.558210 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "================================================================================\n",
      "gists chrow both against mardia king by only air forming bu only a were sunstred\n",
      "ing sout theredey the commath from a project otrial analy ts to dissouna tradity\n",
      "x capifies for list citicular church after stringnary develount to bees demita w\n",
      "ing the instituted supprald markoneletism groups of diring the company fith pies\n",
      "m collection stick to facarnations one five nene name speation silly or was iid \n",
      "================================================================================\n",
      "Validation set perplexity: 4.29\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "with tf.Session(graph=graph,config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True,gpu_options=gpu_options)) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article] (http://arxiv.org/abs/1409.2329)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem 3\n",
    "(difficult!)\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "the quick brown fox\n",
    "\n",
    "the model should attempt to output:\n",
    "eht kciuq nworb xof\n",
    "\n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as this [article](http://arxiv.org/abs/1409.3215) for best practices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
