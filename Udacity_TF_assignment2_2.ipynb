{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (200000, 28, 28), (200000,))\n",
      "('Validation set', (10000, 28, 28), (10000,))\n",
      "('Test set', (18724, 28, 28), (18724,))\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (200000, 784), (200000, 10))\n",
      "('Validation set', (10000, 784), (10000, 10))\n",
      "('Test set', (18724, 784), (18724, 10))\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Baseline for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 19.6592\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 8.2%\n",
      "Minibatch loss at step 500 : 1.56872\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 1000 : 1.39899\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 1500 : 1.10509\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 2000 : 0.902614\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 2500 : 1.09693\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 3000 : 1.19934\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 78.0%\n",
      "Test accuracy: 85.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print \"Initialized\"\n",
    "  for step in xrange(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print \"Minibatch loss at step\", step, \":\", l\n",
    "      print \"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)\n",
    "      print \"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels)\n",
    "  print \"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding regularizition for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "lam = 5e-4\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)+ lam*tf.nn.l2_loss(weights)+lam*tf.nn.l2_loss(biases))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 20.9466\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 10.1%\n",
      "Minibatch loss at step 500 : 2.61517\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 1000 : 2.01302\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 1500 : 1.31996\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 2000 : 1.11361\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 2500 : 1.08071\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 3000 : 0.986267\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 80.3%\n",
      "Test accuracy: 87.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print \"Initialized\"\n",
    "  for step in xrange(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print \"Minibatch loss at step\", step, \":\", l\n",
    "      print \"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)\n",
    "      print \"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels)\n",
    "  print \"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline for 1-layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "node = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, node]))\n",
    "  biases = tf.Variable(tf.zeros([node]))\n",
    "  \n",
    " #### add a hidden layer\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([node,num_labels]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_labels]))  \n",
    "  \n",
    "  # Training computation.\n",
    "  logits_train = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  sigmoid_train = tf.nn.relu(logits_train)\n",
    "  logits_train1 = tf.matmul(sigmoid_train, weights1) + biases1\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits_train1, tf_train_labels))\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits_train1)\n",
    "  logit_val=tf.matmul(tf_valid_dataset, weights) + biases\n",
    "  sigmoid_val = tf.nn.relu(logit_val)\n",
    "  logits_val1 = tf.matmul(sigmoid_val, weights1) + biases1\n",
    "  valid_prediction = tf.nn.softmax(logits_val1)\n",
    "  logit_test=tf.matmul(tf_test_dataset, weights) + biases\n",
    "  sigmoid_test = tf.nn.relu(logit_test)\n",
    "  logits_test1 = tf.matmul(sigmoid_test, weights1) + biases1\n",
    "  test_prediction = tf.nn.softmax(logits_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 361.984\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 30.6%\n",
      "Minibatch loss at step 500 : 42.0269\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 1000 : 9.25481\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 1500 : 11.3963\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 2000 : 6.16731\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2500 : 7.86795\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 3000 : 1.98448\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 82.5%\n",
      "Test accuracy: 89.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print \"Initialized\"\n",
    "  for step in xrange(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print \"Minibatch loss at step\", step, \":\", l\n",
    "      print \"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)\n",
    "      print \"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels)\n",
    "  print \"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add regularization for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "node = 1024\n",
    "lamba = 5e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, node]))\n",
    "  biases = tf.Variable(tf.zeros([node]))\n",
    "  \n",
    " #### add a hidden layer\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([node,num_labels]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_labels]))  \n",
    "  \n",
    "  # Training computation.\n",
    "  logits_train = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  sigmoid_train = tf.nn.relu(logits_train)\n",
    "  logits_train1 = tf.matmul(sigmoid_train, weights1) + biases1\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits_train1, tf_train_labels)+ lamba*tf.nn.l2_loss(weights)+lamba*tf.nn.l2_loss(biases)+ lamba*tf.nn.l2_loss(weights1)+lamba*tf.nn.l2_loss(biases1))\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits_train1)\n",
    "  logit_val=tf.matmul(tf_valid_dataset, weights) + biases\n",
    "  sigmoid_val = tf.nn.relu(logit_val)\n",
    "  logits_val1 = tf.matmul(sigmoid_val, weights1) + biases1\n",
    "  valid_prediction = tf.nn.softmax(logits_val1)\n",
    "  logit_test=tf.matmul(tf_test_dataset, weights) + biases\n",
    "  sigmoid_test = tf.nn.relu(logit_test)\n",
    "  logits_test1 = tf.matmul(sigmoid_test, weights1) + biases1\n",
    "  test_prediction = tf.nn.softmax(logits_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 1882.64\n",
      "Minibatch accuracy: 17.2%\n",
      "Validation accuracy: 20.1%\n",
      "Test accuracy: 21.9%\n",
      "Minibatch loss at step 500 : 965.998\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 80.6%\n",
      "Test accuracy: 87.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 501\n",
    "train_accuracy=[]\n",
    "validation_accuracy=[]\n",
    "test_accuracy= []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print \"Initialized\"\n",
    "  for step in xrange(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      train_accuracy.append(accuracy(predictions, batch_labels))\n",
    "      validation_accuracy.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "      test_accuracy.append(accuracy(test_prediction.eval(), test_labels))\n",
    "      if(step%500 ==0):\n",
    "        print \"Minibatch loss at step\", step, \":\", l\n",
    "        print \"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)\n",
    "        print \"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels)\n",
    "        print \"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEACAYAAABF+UbAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMEvixhiTwCyJG2NMArMkbowxCez/A3jl7PcGZMPXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1109f7d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0,10,1)\n",
    "y = np.vstack([train_accuracy,validation_accuracy,test_accuracy])\n",
    "print y.shape\n",
    "plt.plot(x, y.T[x], linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 : Looking down to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "node = 1024\n",
    "lamba = 5e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, node]))\n",
    "  biases = tf.Variable(tf.zeros([node]))\n",
    "  \n",
    " #### add a hidden layer\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([node,num_labels]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_labels]))  \n",
    "  \n",
    "  # Training computation.\n",
    "  logits_train = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  sigmoid_train = tf.nn.relu(logits_train)\n",
    "  logits_train1 = tf.matmul(sigmoid_train, weights1) + biases1\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits_train1, tf_train_labels)+ lamba*tf.nn.l2_loss(weights)+lamba*tf.nn.l2_loss(biases)+ lamba*tf.nn.l2_loss(weights1)+lamba*tf.nn.l2_loss(biases1))\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits_train1)\n",
    "  logit_val=tf.matmul(tf_valid_dataset, weights) + biases\n",
    "  sigmoid_val = tf.nn.relu(logit_val)\n",
    "  logits_val1 = tf.matmul(sigmoid_val, weights1) + biases1\n",
    "  valid_prediction = tf.nn.softmax(logits_val1)\n",
    "  logit_test=tf.matmul(tf_test_dataset, weights) + biases\n",
    "  sigmoid_test = tf.nn.relu(logit_test)\n",
    "  logits_test1 = tf.matmul(sigmoid_test, weights1) + biases1\n",
    "  test_prediction = tf.nn.softmax(logits_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 1959.95\n",
      "Minibatch accuracy: 4.0%\n",
      "Validation accuracy: 37.2%\n",
      "Minibatch loss at step 500 : 129.86\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 1000 : 11.0844\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 1500 : 1.59171\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 2000 : 0.644427\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 2500 : 0.650042\n",
      "Minibatch accuracy: 84.0%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 3000 : 0.959194\n",
      "Minibatch accuracy: 74.0%\n",
      "Validation accuracy: 82.1%\n",
      "Test accuracy: 89.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print \"Initialized\"\n",
    "  for step in xrange(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print \"Minibatch loss at step\", step, \":\", l\n",
    "      print \"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)\n",
    "      print \"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels)\n",
    "  print \"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on the method of dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 1280\n",
    "node = 1024\n",
    "lamba = 5e-3\n",
    "SEED = 66478\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, node]))\n",
    "  biases = tf.Variable(tf.zeros([node]))\n",
    "  \n",
    " #### add a hidden layer\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([node,num_labels]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_labels]))  \n",
    "  \n",
    "  # Training computation.\n",
    "  logits_train = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  sigmoid_train = tf.nn.relu(logits_train)\n",
    "  dropout = tf.nn.dropout(sigmoid_train,0.5,seed=SEED)\n",
    "  logits_train1 = tf.matmul(dropout, weights1) + biases1\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits_train1, tf_train_labels)+ lamba*tf.nn.l2_loss(weights)+lamba*tf.nn.l2_loss(biases)+ lamba*tf.nn.l2_loss(weights1)+lamba*tf.nn.l2_loss(biases1))\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits_train1)\n",
    "  logit_val=tf.matmul(tf_valid_dataset, weights) + biases\n",
    "  sigmoid_val = tf.nn.relu(logit_val)\n",
    "  logits_val1 = tf.matmul(sigmoid_val, weights1) + biases1\n",
    "  valid_prediction = tf.nn.softmax(logits_val1)\n",
    "  logit_test=tf.matmul(tf_test_dataset, weights) + biases\n",
    "  sigmoid_test = tf.nn.relu(logit_test)\n",
    "  logits_test1 = tf.matmul(sigmoid_test, weights1) + biases1\n",
    "  test_prediction = tf.nn.softmax(logits_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 2039.25\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 35.6%\n",
      "Minibatch loss at step 500 : 126.314\n",
      "Minibatch accuracy: 79.9%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 1000 : 10.7984\n",
      "Minibatch accuracy: 85.8%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 1500 : 1.39295\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 85.7%\n",
      "Test accuracy: 92.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1501\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print \"Initialized\"\n",
    "  for step in xrange(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print \"Minibatch loss at step\", step, \":\", l\n",
    "      print \"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)\n",
    "      print \"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels)\n",
    "  print \"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve model make the accuray as higher as possible(+regulariztion+learning_rate_decay+dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "node_hidden1 = 1024\n",
    "node_hidden2 = 500\n",
    "node_hidden3 = 300\n",
    "node_hidden4 = 50\n",
    "lamba = 5e-3\n",
    "SEED = 66478\n",
    "\n",
    "# 3_layer_NN\n",
    "def NN(dataset, weights, weights1, weights2, weights3,weights4, biases,biases1,biases2,biases3,biases4):\n",
    "  logits_train = tf.matmul(dataset, weights) + biases\n",
    "  sigmoid_train = tf.nn.tanh(logits_train)\n",
    "  #dropout = tf.nn.dropout(sigmoid_train,0.5,seed=SEED)\n",
    "  logits_train1 = tf.matmul(sigmoid_train, weights1) + biases1\n",
    "  sigmoid_train1 = tf.nn.tanh(logits_train1)\n",
    "  #dropout1 = tf.nn.dropout(sigmoid_train1,0.5,seed=SEED)\n",
    "  logits_train2 = tf.matmul(sigmoid_train1, weights2) + biases2\n",
    "  sigmoid_train2 = tf.nn.tanh(logits_train2)\n",
    "  #dropout2 = tf.nn.dropout(sigmoid_train2,0.5,seed=SEED)\n",
    "  logits_train3 = tf.matmul(sigmoid_train2, weights3) + biases3\n",
    "  sigmoid_train3 = tf.nn.tanh(logits_train3)\n",
    "  logits_train4 = tf.matmul(sigmoid_train3, weights4) + biases4\n",
    "  return logits_train4\n",
    "\n",
    "# def NN_non_train(dataset, weights, weights1, weights2,weights3, weights4,biases,biases1,biases2,biases3,biases4):\n",
    "#   logits_train = tf.matmul(dataset, weights) + biases\n",
    "#   sigmoid_train = tf.nn.tanh(logits_train)\n",
    "#   logits_train1 = tf.matmul(sigmoid_train, weights1) + biases1\n",
    "#   sigmoid_train1 = tf.nn.tanh(logits_train1)\n",
    "#   logits_train2 = tf.matmul(sigmoid_train1, weights2) + biases2\n",
    "#   sigmoid_train2 = tf.nn.tanh(logits_train2)\n",
    "#   logits_train3 = tf.matmul(sigmoid_train2, weights3) + biases3\n",
    "#   sigmoid_train3 = tf.nn.tanh(logits_train3)\n",
    "#   logits_train4 = tf.matmul(sigmoid_train3, weights4) + biases4\n",
    "#   return logits_train4\n",
    "    \n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, node_hidden1]))\n",
    "  biases = tf.Variable(tf.zeros([node_hidden1]))\n",
    "  \n",
    " #### add 1st hidden layer\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([node_hidden1,node_hidden2]))\n",
    "  biases1 = tf.Variable(tf.zeros([node_hidden2]))  \n",
    "    \n",
    " #### add 2nd hidden layer\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([node_hidden2,node_hidden3]))\n",
    "  biases2 = tf.Variable(tf.zeros([node_hidden3]))  \n",
    "\n",
    "    \n",
    " #### add 3rd hidden layer\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([node_hidden3,node_hidden4]))\n",
    "  biases3 = tf.Variable(tf.zeros([node_hidden4]))  \n",
    "\n",
    "#### add 4th hidden layer\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([node_hidden4,num_labels]))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    " #### Rate decay\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  learning_rate = tf.train.exponential_decay(0.1, global_step,100000,0.95,staircase=True)\n",
    "\n",
    "  # Training computation.\n",
    "  logits_train4 = NN(tf_train_dataset,weights, weights1, weights2,weights3,weights4,biases,biases1,biases2,biases3,biases4)\n",
    "  L2 = lamba*tf.nn.l2_loss(weights)+lamba*tf.nn.l2_loss(biases)+ lamba*tf.nn.l2_loss(weights1)+lamba*tf.nn.l2_loss(biases1)+ lamba*tf.nn.l2_loss(weights2)+lamba*tf.nn.l2_loss(biases2)+ lamba*tf.nn.l2_loss(weights3)+lamba*tf.nn.l2_loss(biases3)+ lamba*tf.nn.l2_loss(weights4)+lamba*tf.nn.l2_loss(biases4)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits_train4, tf_train_labels)+L2)\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits_train4)\n",
    "  logits_val = NN(tf_valid_dataset, weights, weights1, weights2,weights3,weights4,biases,biases1,biases2,biases3,biases4)\n",
    "  valid_prediction = tf.nn.softmax(logits_val)\n",
    "  logits_test = NN(tf_test_dataset, weights, weights1, weights2,weights3,weights4,biases,biases1,biases2,biases3,biases4)\n",
    "  test_prediction = tf.nn.softmax(logits_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 2868.2\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 10.9%\n",
      "Minibatch loss at step 500 : 1742.81\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 58.5%\n",
      "Minibatch loss at step 1000 : 1057.64\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 1500 : 641.844\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 2000 : 389.51\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 2500 : 236.57\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 3000 : 143.82\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 3500 : 87.4859\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 4000 : 53.0652\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 4500 : 32.3889\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 5000 : 19.8534\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.3%\n",
      "Test accuracy: 91.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print \"Initialized\"\n",
    "  for step in xrange(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print \"Minibatch loss at step\", step, \":\", l\n",
    "      print \"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)\n",
    "      print \"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels)\n",
    "  print \"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
